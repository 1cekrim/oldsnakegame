{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-rla/snakegame/blob/master/model/checkpoint\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oUlGGaWA8gxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2108
        },
        "outputId": "768a8430-27bb-4651-d26f-da4edb98ed3f"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "import os\n",
        "\n",
        "batch_size = 16     #입력으로 한번에 넘겨줄 경험의 수\n",
        "y = 0.99            #타겟 Q에 대한 할인 인자\n",
        "start_e = 1         #무작위 행위 시작 확률\n",
        "min_e = 0.1         #무작위 행위 최종 확률\n",
        "de = 0.99           #e의 감소율\n",
        "episodes = 1000     #에피소드의 갯수\n",
        "max_episode_length = 50 #꼬리잡기 방지\n",
        "path = \"./model\"    #모델 위치\n",
        "load_model = False  #모델을 불러오기 할 것인지\n",
        "model_name = \"model\"#모델의 이름\n",
        "update_freq = 4     #얼마나 자주 훈련할 것인지\n",
        "update_freq_target = 1000 #target이 main과 몇번마다 같아질 것인지\n",
        "\n",
        "jList = []\n",
        "rList = []\n",
        "\n",
        "\n",
        "class Q_net:\n",
        "    def __init__(self, height, width, depth, number_of_possible_actions):\n",
        "        '''\n",
        "        #self.input_data = tf.placeholder(shape = [height * width, depth], dtype = tf.int32)\n",
        "        #self.input_data = tf.reshape(input_data, shape = [height, width, depth])\n",
        "        a = env.get_state()\n",
        "        self.input_data = a.reshape(height, width, depth)\n",
        "        s = ''\n",
        "        for i in range(0, height):\n",
        "            for j in range(0, width):\n",
        "                if (self.input_data[i][j][0] == 1):\n",
        "                    s += '▦ '\n",
        "                elif (self.input_data[i][j][1] == 1 ):\n",
        "                    s += '■ '\n",
        "                elif (self.input_data[i][j][2] == 1):\n",
        "                    s += '☆ '\n",
        "                else:\n",
        "                    s += '　'\n",
        "            s += '\\n'\n",
        "        print(s)\n",
        "        tf.reshape가 예상대로 작동하는지 테스트하는 코드\n",
        "        '''\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.number_of_possible_actions = number_of_possible_actions\n",
        "\n",
        "        input_depth = depth\n",
        "        self.input_data_set = tf.placeholder(shape = [None, height * width, input_depth], dtype = tf.float32)\n",
        "        self.input_data = tf.reshape(self.input_data_set, shape = [-1, height, width, input_depth])\n",
        "\n",
        "        '''\n",
        "        height, width가 들어오면\n",
        "        필터1 : 5x5x32\n",
        "        필터2 : 5x5x32\n",
        "        필터3 : 5x5x32\n",
        "        필터4 : (height - 12)x(height - 12)x256 해서 1x1x256으로 만듦\n",
        "        '''\n",
        "        self.cw1_size = 5\n",
        "        self.cw1_depth = 32\n",
        "        self.cw1 = tf.Variable(tf.random_normal([self.cw1_size, self.cw1_size, input_depth, self.cw1_depth], stddev=1) / np.sqrt(self.cw1_size * self.cw1_size / 2))\n",
        "        self.cb1 = tf.Variable(tf.zeros([self.cw1_depth]))\n",
        "        self.conv1 = tf.nn.conv2d(self.input_data, self.cw1, strides = [1, 1, 1, 1], padding='VALID') + self.cb1\n",
        "        self.conv1 = tf.nn.relu(self.conv1)\n",
        "\n",
        "        self.cw2_size = 5\n",
        "        self.cw2_depth = 64\n",
        "        self.cw2 = tf.Variable(tf.random_normal([self.cw2_size, self.cw2_size, self.cw1_depth, self.cw2_depth], stddev=1) / np.sqrt(self.cw2_size * self.cw2_size / 2))\n",
        "        self.cb2 = tf.Variable(tf.zeros([self.cw2_depth]))\n",
        "        self.conv2 = tf.nn.conv2d(self.conv1, self.cw2, strides = [1, 1, 1, 1], padding='VALID') + self.cb2\n",
        "        self.conv2 = tf.nn.relu(self.conv2)\n",
        "\n",
        "        self.cw3_size = 5\n",
        "        self.cw3_depth = 128\n",
        "        self.cw3 = tf.Variable(tf.random_normal([self.cw3_size, self.cw3_size, self.cw2_depth, self.cw3_depth], stddev=1) / np.sqrt(self.cw3_size * self.cw3_size / 2))\n",
        "        self.cb3 = tf.Variable(tf.zeros([self.cw3_depth]))\n",
        "        self.conv3 = tf.nn.conv2d(self.conv2, self.cw3, strides = [1, 1, 1, 1], padding='VALID') + self.cb3\n",
        "        self.conv3 = tf.nn.relu(self.conv3)\n",
        "        \n",
        "        self.cw4_size = self.height - 12\n",
        "        self.cw4_depth = 256\n",
        "        self.cw4 = tf.Variable(tf.random_normal([self.cw4_size, self.cw4_size, self.cw3_depth, self.cw4_depth], stddev=1) / np.sqrt(self.cw4_size * self.cw4_size / 2))\n",
        "        self.cb4 = tf.Variable(tf.zeros([self.cw4_depth]))\n",
        "        self.conv4 = tf.nn.conv2d(self.conv3, self.cw4, strides = [1, 1, 1, 1], padding='VALID') + self.cb4\n",
        "        self.conv4 = tf.nn.relu(self.conv4)\n",
        "        \n",
        "        '''\n",
        "        wa = action의 가치를 계산 하는 신경망\n",
        "        wv = 게임 전체의 유리함을 판단하는 신경망\n",
        "        '''\n",
        "        self.stream = tf.reshape(self.conv4, shape = [-1, self.cw4_depth])\n",
        "        self.wa = tf.Variable(tf.random_normal([self.cw4_depth, self.number_of_possible_actions], stddev=1) / np.sqrt(self.cw4_depth / 2))\n",
        "        self.wv = tf.Variable(tf.random_normal([self.cw4_depth, 1], stddev=1) / np.sqrt(self.cw4_depth / 2))\n",
        "\n",
        "        self.adventage = tf.matmul(self.stream, self.wa)\n",
        "        self.value = tf.matmul(self.stream, self.wv)\n",
        "\n",
        "        self.Q = self.value + tf.subtract(self.adventage, tf.reduce_mean(self.adventage, reduction_indices = 1, keep_dims = True))\n",
        "        self.selected_action = tf.argmax(self.Q, 1)\n",
        "\n",
        "        self.targetQ = tf.placeholder(shape = [None], dtype = tf.float32)\n",
        "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
        "        self.actions_onehot = tf.one_hot(self.actions, self.number_of_possible_actions, dtype=tf.float32)\n",
        "        self.Q_action = tf.reduce_sum(tf.multiply(self.Q, self.actions_onehot), axis=1) # reduction_indices=1, tf.one_hot([ad for ad in range(0, self.number_of_possible_actions)]\n",
        "\n",
        "        #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "        #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
        "        self.error = tf.square(self.targetQ - self.Q_action)\n",
        "        self.loss = tf.reduce_mean(self.error)\n",
        "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "        self.updateModel = self.trainer.minimize(self.loss)\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self, max_memory_size, height, width, depth):\n",
        "        self.max_memory_size = max_memory_size\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.mem_state= np.zeros((max_memory_size, height * width, depth))\n",
        "        self.mem_state_new= np.zeros((max_memory_size, height * width, depth))\n",
        "        self.mem_reward = np.zeros(max_memory_size)\n",
        "        self.mem_action = np.zeros(max_memory_size)\n",
        "        self.mem_end = np.zeros(max_memory_size)\n",
        "        self.index = 0\n",
        "        self.max_index = 0\n",
        "\n",
        "    def save(self, state, action, reward, state_new, end):\n",
        "        self.mem_state[self.index] = state\n",
        "        self.mem_state_new[self.index] = state_new\n",
        "        self.mem_reward[self.index] = reward\n",
        "        self.mem_action[self.index] = action\n",
        "        self.mem_end[self.index] = end\n",
        "        self.index += 1\n",
        "        if self.max_index < self.max_memory_size:\n",
        "            self.max_index += 1 \n",
        "        if (self.index >= self.max_memory_size):\n",
        "            self.index = 0\n",
        "\n",
        "    def load(self, num):\n",
        "        if num > self.max_index:\n",
        "            num = self.max_index\n",
        "        k = rnd.sample(range(0, self.max_index), num)\n",
        "        action = np.empty(num)\n",
        "        reward = np.empty(num)\n",
        "        end = np.empty(num)\n",
        "        state = np.empty((num, self.height * self.width, self.depth))\n",
        "        state_new = np.empty((num, self.height * self.width, self.depth))\n",
        "        for d in range(0, num):\n",
        "            state[d] = self.mem_state[k[d]]\n",
        "            state_new[d] = self.mem_state_new[k[d]]\n",
        "            action[d] = self.mem_action[k[d]]\n",
        "            reward[d] = self.mem_reward[k[d]]\n",
        "            end[d] = self.mem_end[k[d]]\n",
        "        return state, action, reward, state_new, end\n",
        "      \n",
        "class Snake:\n",
        "    number_of_possible_actions = 3\n",
        "    padding = 2\n",
        "    depth = 3\n",
        "    length = 5\n",
        "    def __init__(self, height, width, number_of_food):\n",
        "        self.height = height + 2\n",
        "        self.width = width + 2\n",
        "        self.number_of_food = number_of_food\n",
        "        self.init_env()\n",
        "\n",
        "    def init_env(self):\n",
        "        self.board = np.zeros((3, self.height, self.width))\n",
        "        self.direction = 0\n",
        "        self.body = []\n",
        "        self.length = 5\n",
        "        \n",
        "        for i in range(0, self.width):\n",
        "            self.board[0][0][i] = 1\n",
        "            self.board[0][self.height - 1][i] = 1\n",
        "        \n",
        "        for i in range(1, self.height):\n",
        "            self.board[0][i][0] = 1\n",
        "            self.board[0][i][self.width - 1] = 1\n",
        "\n",
        "        for i in range(0, self.length):\n",
        "            self.body.append([self.height // 2, self.width // 2 - i])\n",
        "            self.board[1][self.height // 2][self.width // 2 - i] = 1\n",
        "\n",
        "        for i in range(0, self.number_of_food):\n",
        "            self.put_food_random()\n",
        "\n",
        "        \n",
        "\n",
        "    def get_state(self):\n",
        "        h = self.height + 4\n",
        "        w = self.width + 4\n",
        "        reshaped = np.zeros((3, h * w))\n",
        "        for i in range(0, 3):\n",
        "            t = np.pad(self.board[i], ((2, 2), (2, 2)), 'constant', constant_values = (0))\n",
        "            reshaped[i] = t.reshape(h * w)\n",
        "        return reshaped.T\n",
        "\n",
        "    def show_board(self):\n",
        "        s = ''\n",
        "        for i in range(0, self.height):\n",
        "            for j in range(0, self.width):\n",
        "                if (self.board[0][i][j] == 1):\n",
        "                    s += '▦ '\n",
        "                elif (self.board[1][i][j] == 1):\n",
        "                    if (i == self.body[0][0] and j == self.body[0][1]):\n",
        "                        s += '● '\n",
        "                    else:    \n",
        "                        s += '■ '\n",
        "                elif (self.board[2][i][j] == 1):\n",
        "                    s += '☆ '\n",
        "                else:\n",
        "                    s += '　'\n",
        "            s += '\\n'\n",
        "        \n",
        "        print(s)\n",
        "\n",
        "    def put_food(self, y, x):\n",
        "        self.board[2][y][x] = 1\n",
        "\n",
        "    def put_food_random(self):\n",
        "        while True:\n",
        "            y, x = rnd.randrange(1, self.height - 1), rnd.randrange(1, self.width - 1)\n",
        "            if (self.board[2][y][x] == 0 and self.board[1][y][x] == 0):\n",
        "                break\n",
        "        self.put_food(y, x)\n",
        "\n",
        "    \n",
        "\n",
        "    def do_action(self, action):\n",
        "        is_ended = False\n",
        "        dir = [[0, 1], [1, 0], [0, -1], [-1, 0]]\n",
        "        if (action == 0):\n",
        "            #좌회전\n",
        "            self.direction -= 1\n",
        "        elif (action == 2):\n",
        "            #우회전\n",
        "            self.direction += 1\n",
        "            \n",
        "        if (self.direction < 0):\n",
        "            self.direction = 3\n",
        "        elif (self.direction > 3):\n",
        "            self.direction = 0\n",
        "        \n",
        "        head = self.body[0]\n",
        "        head = [head[0] + dir[self.direction][0], head[1] + dir[self.direction][1]]\n",
        "        \n",
        "        i = head[0]\n",
        "        j = head[1]\n",
        "\n",
        "        reward = -0.5\n",
        "\n",
        "        if (self.board[0][i][j] == 1):\n",
        "            reward = -1\n",
        "            is_ended = True\n",
        "        elif (self.board[1][i][j] == 1):\n",
        "            reward = -1\n",
        "            is_ended = True\n",
        "        elif (self.board[2][i][j] == 1):\n",
        "            self.length += 1\n",
        "            reward = self.length\n",
        "            self.put_food_random()\n",
        "        else:\n",
        "            reward = -0.1\n",
        "\n",
        "        self.body.insert(0, head)\n",
        "        self.board[0][self.body[0][0]][self.body[0][1]] = 0\n",
        "        self.board[1][self.body[0][0]][self.body[0][1]] = 1\n",
        "        self.board[2][self.body[0][0]][self.body[0][1]] = 0\n",
        "        \n",
        "        if (reward != self.length):\n",
        "            self.board[1][self.body[self.length][0]][self.body[self.length][1]] = 0\n",
        "            del self.body[self.length]\n",
        "        \n",
        "            \n",
        "\n",
        "        return self.get_state(), action, reward, is_ended\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.padding = env.padding\n",
        "        self.depth = env.depth\n",
        "        self.number_of_possible_actions = env.number_of_possible_actions\n",
        "        self.height = env.height + 2 * self.padding\n",
        "        self.width = env.width + 2 * self.padding\n",
        "\n",
        "    def get_copy_var_ops(self, *, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
        "        '''타겟네트워크에 메인네트워크의 Weight값을 복사.\n",
        "        Args: dest_scope_name=\"target\"(DQN): 'target'이라는 이름을 가진 객체를 가져옴 \n",
        "        src_scope_name=\"main\"(DQN): 'main'이라는 이름을 가진 객체를 가져옴 \n",
        "        Returns: list: main의 trainable한 값들이 target의 값으로 복사된 값\n",
        "        출처: https://passi0n.tistory.com/88 [웅이의 공간]''' \n",
        "        op_holder = [] \n",
        "        src_vars = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name) \n",
        "        dest_vars = tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name) \n",
        "        for src_var, dest_var in zip(src_vars, dest_vars): \n",
        "            op_holder.append(dest_var.assign(src_var.value())) \n",
        "        return op_holder\n",
        "\n",
        "\n",
        "    def save(self, sess, name):\n",
        "        print(\"저장 시작\")\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(sess, path + \"/\" + name + \".ckpt\")\n",
        "\n",
        "    def load(self, sess, name):\n",
        "        print(\"불러오기 시작\")\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            print(name + \".ckpt가 없습니다\")\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, path + \"/\" + name + \".ckpt\")\n",
        "\n",
        "    def play(self):\n",
        "        tf.reset_default_graph()\n",
        "        main_net = Q_net(self.height, self.width, self.depth, self.number_of_possible_actions)\n",
        "        target_net = Q_net(self.height, self.width, self.depth, self.number_of_possible_actions)\n",
        "        init = tf.global_variables_initializer()\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "            if load_model == True:\n",
        "                self.load(sess, model_name)\n",
        "\n",
        "            for i in range(episodes):\n",
        "                self.env.init_env()\n",
        "                j = 0\n",
        "                s = self.env.get_state()\n",
        "                while j < max_episode_length:\n",
        "                    j += 1\n",
        "                    act = sess.run(main_net.selected_action, feed_dict={main_net.input_data_set: [s]})[0]\n",
        "                    st, a, r, end = self.env.do_action(act)\n",
        "                    self.env.show_board()\n",
        "                    s = st\n",
        "                    if end:\n",
        "                        break\n",
        "\n",
        "    def train(self):\n",
        "        tf.reset_default_graph()\n",
        "        main_net = Q_net(self.height, self.width, self.depth, self.number_of_possible_actions)\n",
        "        target_net = Q_net(self.height, self.width, self.depth, self.number_of_possible_actions)\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        e = start_e\n",
        "        steps = 0\n",
        "        r_all = 0\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "            if load_model == True:\n",
        "                self.load(sess, model_name)\n",
        "            sess.run(self.get_copy_var_ops(dest_scope_name=\"target_net\", src_scope_name=\"main_net\"))\n",
        "\n",
        "            for i in range(episodes):\n",
        "                mem = Memory(1000, self.height, self.width, self.depth)\n",
        "                self.env.init_env()\n",
        "                j = 0\n",
        "                s = self.env.get_state()\n",
        "                while j < max_episode_length:\n",
        "                    j += 1\n",
        "                    if np.random.rand(1) < e:\n",
        "                        act = np.random.randint(0, self.number_of_possible_actions)\n",
        "                    else:\n",
        "                        act = sess.run(main_net.selected_action, feed_dict={main_net.input_data_set: [s]})[0]\n",
        "                    st, a, r, end = self.env.do_action(act)\n",
        "                    mem.save(s, a, r, st, end)\n",
        "                    steps += 1\n",
        "                    if e > min_e:\n",
        "                        e *= de\n",
        "                    elif steps % update_freq == 0:\n",
        "                        bs = min(batch_size, mem.max_index)\n",
        "                        state_batch, action_batch, reward_batch, state_new_batch, end_batch = mem.load(bs)\n",
        "\n",
        "                        Q1 = sess.run(main_net.selected_action, feed_dict={main_net.input_data_set: state_new_batch})\n",
        "                        Q2 = sess.run(target_net.Q, feed_dict={target_net.input_data_set: state_new_batch})\n",
        "                        dQ = Q2[range(bs), Q1]\n",
        "                        em = []\n",
        "                        for k in range(0, bs):\n",
        "                            if not end_batch[k]:\n",
        "                                em.append(1)\n",
        "                            else:\n",
        "                                em.append(0)\n",
        "                        tQ = reward_batch + (y * dQ * em)\n",
        "                        _ = sess.run(main_net.updateModel, \\\n",
        "                            feed_dict={main_net.input_data_set: state_batch, main_net.targetQ: tQ, main_net.actions: action_batch})\n",
        "                    if e <= min_e and steps % update_freq_target == 0:\n",
        "                        sess.run(self.get_copy_var_ops(dest_scope_name=\"target_net\", src_scope_name=\"main_net\"))\n",
        "                    r_all += r\n",
        "                    s = st\n",
        "                    if end:\n",
        "                        break\n",
        "\n",
        "                jList.append(j)\n",
        "                rList.append(r_all)\n",
        "                if (i % (episodes // 10) == 0):\n",
        "                    self.save(sess, model_name)\n",
        "                if len(rList) % 10 == 0:\n",
        "                    print(steps, np.mean(rList[-10:]), e)\n",
        "            self.save(sess, model_name)\n",
        "        print(\"완료: \" + str(sum(rList) / episodes))\n",
        "        \n",
        "height = 15\n",
        "width = 15\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    snake = Snake(height, width, 10)\n",
        "\n",
        "    ag = Agent(snake)\n",
        "\n",
        "    ag.train()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "저장 시작\n",
            "156 34.569999999999986 0.20849246173476127\n",
            "255 73.28000000000017 0.09910481551887473\n",
            "335 87.2100000000006 0.09910481551887473\n",
            "416 103.91000000000096 0.09910481551887473\n",
            "513 99.72000000000142 0.09910481551887473\n",
            "616 109.06000000000195 0.09910481551887473\n",
            "735 125.92000000000246 0.09910481551887473\n",
            "849 130.48000000000303 0.09910481551887473\n",
            "907 131.67000000000343 0.09910481551887473\n",
            "1025 165.08000000000385 0.09910481551887473\n",
            "저장 시작\n",
            "1103 182.14000000000425 0.09910481551887473\n",
            "1187 202.70000000000465 0.09910481551887473\n",
            "1301 214.49000000000507 0.09910481551887473\n",
            "1365 228.1700000000056 0.09910481551887473\n",
            "1469 235.2500000000059 0.09910481551887473\n",
            "1544 241.59000000000637 0.09910481551887473\n",
            "1602 240.5500000000067 0.09910481551887473\n",
            "1710 241.41000000000707 0.09910481551887473\n",
            "1825 278.6200000000064 0.09910481551887473\n",
            "1911 289.19000000000466 0.09910481551887473\n",
            "저장 시작\n",
            "2029 293.81000000000233 0.09910481551887473\n",
            "2126 293.7200000000005 0.09910481551887473\n",
            "2195 288.1799999999988 0.09910481551887473\n",
            "2264 277.0499999999975 0.09910481551887473\n",
            "2350 270.6499999999961 0.09910481551887473\n",
            "2429 292.2999999999943 0.09910481551887473\n",
            "2505 303.5899999999929 0.09910481551887473\n",
            "2604 318.2199999999914 0.09910481551887473\n",
            "2692 336.3999999999895 0.09910481551887473\n",
            "2740 334.8599999999884 0.09910481551887473\n",
            "저장 시작\n",
            "2807 324.5399999999874 0.09910481551887473\n",
            "2911 330.2599999999858 0.09910481551887473\n",
            "3017 338.13999999998356 0.09910481551887473\n",
            "3114 341.5899999999814 0.09910481551887473\n",
            "3191 342.1099999999798 0.09910481551887473\n",
            "3285 346.2399999999785 0.09910481551887473\n",
            "3377 342.0799999999764 0.09910481551887473\n",
            "3509 361.7699999999741 0.09910481551887473\n",
            "3606 385.17999999997204 0.09910481551887473\n",
            "3742 389.62999999996947 0.09910481551887473\n",
            "저장 시작\n",
            "3856 392.9799999999673 0.09910481551887473\n",
            "3950 413.8799999999652 0.09910481551887473\n",
            "4110 429.4899999999623 0.09910481551887473\n",
            "4208 431.5299999999602 0.09910481551887473\n",
            "4350 476.1699999999576 0.09910481551887473\n",
            "4449 489.72999999995534 0.09910481551887473\n",
            "4531 496.25999999995355 0.09910481551887473\n",
            "4640 502.2399999999519 0.09910481551887473\n",
            "4710 515.92999999995 0.09910481551887473\n",
            "4796 536.0199999999487 0.09910481551887473\n",
            "저장 시작\n",
            "4923 542.1699999999466 0.09910481551887473\n",
            "5021 545.2299999999442 0.09910481551887473\n",
            "5124 548.0099999999422 0.09910481551887473\n",
            "5222 543.6899999999405 0.09910481551887473\n",
            "5305 549.0499999999384 0.09910481551887473\n",
            "5384 565.7699999999371 0.09910481551887473\n",
            "5501 573.3599999999351 0.09910481551887473\n",
            "5603 585.409999999933 0.09910481551887473\n",
            "5729 605.2599999999309 0.09910481551887473\n",
            "5819 610.2399999999286 0.09910481551887473\n",
            "저장 시작\n",
            "5939 605.0999999999266 0.09910481551887473\n",
            "6042 625.1699999999244 0.09910481551887473\n",
            "6180 639.6799999999218 0.09910481551887473\n",
            "6314 658.9199999999189 0.09910481551887473\n",
            "6397 666.9399999999173 0.09910481551887473\n",
            "6494 682.6799999999155 0.09910481551887473\n",
            "6608 692.7699999999136 0.09910481551887473\n",
            "6724 693.5199999999111 0.09910481551887473\n",
            "6819 693.849999999909 0.09910481551887473\n",
            "6948 697.2099999999065 0.09910481551887473\n",
            "저장 시작\n",
            "7063 696.3299999999044 0.09910481551887473\n",
            "7163 692.3199999999022 0.09910481551887473\n",
            "7261 700.1999999999001 0.09910481551887473\n",
            "7397 710.7899999998979 0.09910481551887473\n",
            "7542 703.9299999998951 0.09910481551887473\n",
            "7679 728.709999999892 0.09910481551887473\n",
            "7786 765.3899999998898 0.09910481551887473\n",
            "7884 770.3999999998875 0.09910481551887473\n",
            "7998 783.9299999998856 0.09910481551887473\n",
            "8096 788.3699999998837 0.09910481551887473\n",
            "저장 시작\n",
            "8201 793.7699999998817 0.09910481551887473\n",
            "8312 793.7399999998795 0.09910481551887473\n",
            "8421 815.1999999998774 0.09910481551887473\n",
            "8536 870.0799999998753 0.09910481551887473\n",
            "8659 883.1199999998728 0.09910481551887473\n",
            "8768 908.1999999998704 0.09910481551887473\n",
            "8861 925.1099999998689 0.09910481551887473\n",
            "8929 923.0799999998671 0.09910481551887473\n",
            "9008 922.1499999998657 0.09910481551887473\n",
            "9133 927.3499999998639 0.09910481551887473\n",
            "저장 시작\n",
            "9228 936.8899999998619 0.09910481551887473\n",
            "9322 949.9399999998601 0.09910481551887473\n",
            "9457 965.0699999998575 0.09910481551887473\n",
            "9596 976.2999999998547 0.09910481551887473\n",
            "9730 1002.4699999998522 0.09910481551887473\n",
            "9851 1017.6399999998506 0.09910481551887473\n",
            "9952 1032.3299999998555 0.09910481551887473\n",
            "10016 1019.9399999998601 0.09910481551887473\n",
            "10067 1009.5899999998594 0.09910481551887473\n",
            "10118 1015.3399999998586 0.09910481551887473\n",
            "저장 시작\n",
            "완료: 529.6126999999415\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}